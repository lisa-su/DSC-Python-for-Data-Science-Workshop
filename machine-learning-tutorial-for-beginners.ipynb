{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dcc6c619-c02a-4bea-b322-4f32779ae672",
    "_uuid": "819c6b57207e720308ea29491d85a6e8af8c253e"
   },
   "source": [
    "\\*Contents in this Jupyter Notebook are from and can be found in [DATAI's kaggle kernel](https://www.kaggle.com/kanncaa1). The order of the content were changed for this workshop session.\n",
    "\n",
    "Data scientist need to have these skills:\n",
    "1. Basic Tools: Like python, R or SQL. You do not need to know everything. What you only need is to learn how to use **python**\n",
    "1. Basic Statistics: Like mean, median or standart deviation. If you know basic statistics, you can use **python** easily. \n",
    "1. Data Munging: Working with messy and difficult data. Like a inconsistent date and string formatting. As you guess, **python** helps us.\n",
    "1. Data Visualization: Title is actually explanatory. We will visualize the data with **python** like matplot and seaborn libraries.\n",
    "1. Machine Learning: You do not need to understand math behind the machine learning technique. You only need is understanding basics of machine learning and learning how to implement it while using **python**.\n",
    "\n",
    "**Content:**\n",
    "1. Introduction to Python:\n",
    "    1. Matplotlib\n",
    "    1. Dictionaries \n",
    "    1. Pandas\n",
    "    1. Logic, control flow and filtering\n",
    "    1. Loop data structures\n",
    "1. Python Data Science Toolbox:\n",
    "    1. User defined function \n",
    "    1. Scope\n",
    "    1. Nested function\n",
    "    1. Default and flexible arguments\n",
    "    1. Lambda function\n",
    "    1. Anonymous function\n",
    "    1. Iterators\n",
    "    1. List comprehension\n",
    "1. Cleaning Data\n",
    "    1. Diagnose data for cleaning\n",
    "    1. Explotary data analysis\n",
    "    1. Visual exploratory data analysis\n",
    "    1. Tidy data\n",
    "    1. Pivoting data\n",
    "    1. Concatenating data\n",
    "    1. Data types\n",
    "    1. Missing data and testing with assert\n",
    "1. Pandas Foundation\n",
    "    1. Review of pandas\n",
    "    1. Building data frames from scratch\n",
    "    1. Visual exploratory data analysis\n",
    "    1. Statistical explatory data analysis\n",
    "    1. Indexing pandas time series\n",
    "    1. Resampling pandas time series\n",
    "1. Manipulating Data Frames with Pandas\n",
    "    1. Indexing data frames\n",
    "    1. Slicing data frames\n",
    "    1. Filtering data frames\n",
    "    1. Transforming data frames\n",
    "    1. Index objects and labeled data\n",
    "    1. Hierarchical indexing\n",
    "    1. Pivoting data frames\n",
    "    1. Stacking and unstacking data frames\n",
    "    1. Melting data frames\n",
    "    1. Categoricals and groupby\n",
    "1. Data Visualization\n",
    "    1. Seaborn: https://www.kaggle.com/kanncaa1/seaborn-for-beginners\n",
    "    1. Bokeh: https://www.kaggle.com/kanncaa1/interactive-bokeh-tutorial-part-1\n",
    "    1. Bokeh: https://www.kaggle.com/kanncaa1/interactive-bokeh-tutorial-part-2\n",
    "1. Statistical Thinking\n",
    "    1. https://www.kaggle.com/kanncaa1/basic-statistic-tutorial-for-beginners\n",
    "1. [Machine Learning](#1)\n",
    "    1. [Supervised Learning](#2)\n",
    "        1. [EDA(Exploratory Data Analysis)](#3)\n",
    "        1. [K-Nearest Neighbors (KNN)](#4)\n",
    "        1. [Regression](#5)\n",
    "        1. [Cross Validation (CV)](#6)\n",
    "        1. [ROC Curve](#7)\n",
    "        1. [Hyperparameter Tuning](#8)\n",
    "        1. [Pre-procesing Data](#9)\n",
    "    1. [Unsupervised Learning](#10)\n",
    "        1. [Kmeans Clustering](#11)\n",
    "        1. [Evaluation of Clustering](#12)\n",
    "        1. [Standardization](#13)\n",
    "        1. [Hierachy](#14)\n",
    "        1. [T - Distributed Stochastic Neighbor Embedding (T - SNE)](#15)\n",
    "        1. [Principle Component Analysis (PCA)](#16)\n",
    "1. Deep Learning\n",
    "    1. https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners\n",
    "1. Time Series Prediction\n",
    "    1. https://www.kaggle.com/kanncaa1/time-series-prediction-tutorial-with-eda\n",
    "1. Deep Learning with Pytorch\n",
    "    1. Artificial Neural Network: https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers\n",
    "    1. Convolutional Neural Network: https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers\n",
    "    1. Recurrent Neural Network: https://www.kaggle.com/kanncaa1/recurrent-neural-network-with-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5ee3a7aa-eca4-411b-9f84-d14c09e13730",
    "_uuid": "2b90d6250c8f9c2c302c849bffa132bd3483e893"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "# import warnings\n",
    "import warnings\n",
    "# ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "32af03f6-41be-41ec-9023-8cd519040984",
    "_uuid": "a9c5426e9e5cef81c1e1639ebe57e9b45dfd2c43"
   },
   "outputs": [],
   "source": [
    "# read csv (comma separated value) into data\n",
    "data = pd.read_csv('../input/column_2C_weka.csv')\n",
    "print(plt.style.available) # look at available plot styles\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "89a724e2-426d-427b-9107-06835010cf59",
    "_uuid": "f4fa42e9a6cf069d54459be42a1726ab03c2f1e5"
   },
   "source": [
    "<a id=\"1\"></a> <br>\n",
    "# 8. MACHINE LEARNING (ML)\n",
    "In python there are some ML libraries like sklearn, keras or tensorflow. We will use sklearn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "10f3b719-c44b-451a-b464-0adf4e1e1522",
    "_uuid": "f6a2b205e1e3fc647bc2ee88d702b8572bf1cc75"
   },
   "source": [
    "<a id=\"2\"></a> <br>\n",
    "## A. SUPERVISED LEARNING\n",
    "* Supervised learning: It uses data that has labels. Example, there are orthopedic patients data that have labels *normal* and *abnormal*.\n",
    "    * There are features(predictor variable) and target variable. Features are like *pelvic radius* or *sacral slope*(If you have no idea what these are like me, you can look images in google like what I did :) )Target variables are labels *normal* and *abnormal*\n",
    "    * Aim is that as given features(input) predict whether target variable(output) is *normal* or *abnormal*\n",
    "    * Classification: target variable consists of categories like normal or abnormal\n",
    "    * Regression: target variable is continious like stock market\n",
    "    * If these explanations are not enough for you, just google them. However, be careful about terminology: features = predictor variable = independent variable = columns = inputs. target variable = responce variable = class = dependent variable = output = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "65e897a1-8259-44c5-9cb7-e5e653f9032d",
    "_uuid": "a0e671bf2ef8dbe81da2705ad70b69401bb7af16"
   },
   "source": [
    "<a id=\"3\"></a> <br>\n",
    "### EXPLORATORY DATA ANALYSIS (EDA)\n",
    "* In order to make something in data, as you know you need to explore data. Detailed exploratory data analysis is in my Data Science Tutorial for Beginners\n",
    "* I always start with *head()* to see features that are *pelvic_incidence,\tpelvic_tilt numeric,\tlumbar_lordosis_angle,\tsacral_slope,\tpelvic_radius* and \t*degree_spondylolisthesis* and target variable that is *class*\n",
    "* head(): default value of it shows first 5 rows(samples). If you want to see for example 100 rows just write head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c1ecd622-67cc-485f-bfa7-8c682d30a5eb",
    "_uuid": "9a5993f4962882e1156f2062b7abf706a0739d51"
   },
   "outputs": [],
   "source": [
    "# to see features and target variable\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1631690c-bb9d-4460-a7d9-a335aa914b4f",
    "_uuid": "b7b9addc824de1a35b67d96d3092ffcb10869947"
   },
   "outputs": [],
   "source": [
    "# Well know question is is there any NaN value and length of this data so lets look at info\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a7dd2a6f-a81d-4dce-9d74-fd0148c446ae",
    "_uuid": "96f97c33305956eb76d8a2043fd71aff05e38548"
   },
   "source": [
    "As you can see:\n",
    "* length: 310 (range index)\n",
    "* Features are float\n",
    "* Target variables are object that is like string\n",
    "* Okey we have some ideas about data but lets look go inside data deeper\n",
    "    * describe(): I explain it in previous tutorial so there is a Quiz :)\n",
    "        * Why we need to see statistics like mean, std, max or min? I hate from quizzes :) so answer: In order to visualize data, values should be closer each other. As you can see values looks like closer. At least there is no incompatible values like mean of one feature is 0.1 and other is 1000. Also there are another reasons that I will mention next parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "137897ca-b519-4ac3-afdd-6c4136447e39",
    "_uuid": "69d132068cce6a915aac7678b4e9fbcf3e365643"
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3776dd3d-d0aa-419e-b788-e75454e94b86",
    "_uuid": "c8961b1f3a3a73547a0b7d27955f9844f6ad43eb"
   },
   "source": [
    "pd.plotting.scatter_matrix:\n",
    "* green: *normal* and red: *abnormal*\n",
    "* c:  color\n",
    "* figsize: figure size\n",
    "* diagonal: histohram of each features\n",
    "* alpha: opacity\n",
    "* s: size of marker\n",
    "* marker: marker type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fb106765-bb47-452b-8d6e-3578b479873c",
    "_uuid": "5dc0763dde8b2638a5289f0b4496f384f85aca85"
   },
   "outputs": [],
   "source": [
    "color_list = ['red' if i=='Abnormal' else 'green' for i in data.loc[:,'class']]\n",
    "pd.plotting.scatter_matrix(data.loc[:, data.columns != 'class'],\n",
    "                                       c=color_list,\n",
    "                                       figsize= [15,15],\n",
    "                                       diagonal='hist',\n",
    "                                       alpha=0.5,\n",
    "                                       s = 200,\n",
    "                                       marker = '*',\n",
    "                                       edgecolor= \"black\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "53fc7ab6-de8b-4b8f-9e4a-38c5db72eea0",
    "_uuid": "d77cef37b8c5c4f07d3f4aa94cc4ad1ccbd7caca"
   },
   "source": [
    "Okay, as you understand in scatter matrix there are relations between each feature but how many *normal(green)* and *abnormal(red)* classes are there. \n",
    "* Searborn library has *countplot()* that counts number of classes\n",
    "* Also you can print it with *value_counts()* method\n",
    "\n",
    "<br> This data looks like balanced. Actually there is no definiton or numeric value of balanced data but this data is balanced enough for us.\n",
    "<br> Now lets learn first classification method KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "36243fa5-1fa6-4f8b-bc16-43449b0dc898",
    "_uuid": "e1bb9fd338e6900888e2e4717f54be46cee848a2"
   },
   "outputs": [],
   "source": [
    "sns.countplot(x=\"class\", data=data)\n",
    "data.loc[:,'class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "24a5d90f-3e7d-4733-a6f9-ff4f51145155",
    "_uuid": "9263c479815bb729dad40bf01b68aa18a3c946ac"
   },
   "source": [
    "<a id=\"4\"></a> <br>\n",
    "###  K-NEAREST NEIGHBORS (KNN)\n",
    "https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n",
    "* KNN: Look at the K closest labeled data points\n",
    "* Classification method.\n",
    "* First we need to train our data. Train = fit\n",
    "* fit(): fits the data, train the data.\n",
    "* predict(): predicts the data\n",
    "<br> If you do not understand what is KNN, look at youtube there are videos like 4-5 minutes. You can understand better with it.\n",
    "<br> Lets learn how to implement it with sklearn\n",
    "* x: features\n",
    "* y: target variables(normal, abnormal)\n",
    "* n_neighbors: K. In this example it is 3. it means that Look at the 3 closest labeled data points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c717491d-2bd5-4dc7-ac13-b9f581b1cddd",
    "_uuid": "854f0a3898a928640b9714fcd584e48c9b377f9f"
   },
   "outputs": [],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "x,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\n",
    "knn.fit(x,y)\n",
    "prediction = knn.predict(x)\n",
    "print('Prediction: {}'.format(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b5d85f4e-ab30-4c49-b2bc-6265b6baea9d",
    "_uuid": "6d9c3eacd279ddf7c0ef33b9e1814cd549d0feaa"
   },
   "source": [
    "We have fit the data and predict it with KNN. \n",
    "\n",
    "But there's a BIG problem in this process - the model is trained with `x` and again predict with `x`.\n",
    "\n",
    "<br>We need to split our data train and test sets.\n",
    "* train: use train set by fitting\n",
    "* test: make prediction on test set.\n",
    "* With train and test sets, fitted data and tested data are completely different\n",
    "* train_test_split(x,y,test_size = 0.3,random_state = 1)\n",
    "    * x: features\n",
    "    * y: target variables (normal,abnormal)\n",
    "    * test_size: percentage of test size. Example test_size = 0.3, test size = 30% and train size = 70%\n",
    "    * random_state: sets a seed. If this seed is same number, train_test_split() produce exact same split at each time\n",
    "* fit(x_train,y_train): fit on train sets\n",
    "* score(x_test,y_test)): predict and give accuracy on test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79865658-c89f-43c8-a1a9-75acc4feab6a",
    "_uuid": "4702429fdfa62650937b09fde5a8fd3136da8c55"
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1)\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "x,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\n",
    "knn.fit(x_train,y_train)\n",
    "prediction = knn.predict(x_test)\n",
    "#print('Prediction: {}'.format(prediction))\n",
    "print('With KNN (K=3) accuracy is: ',knn.score(x_test,y_test)) # accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a5665258-3f7f-435a-a634-49eb0c0d66e0",
    "_uuid": "544f51ef05efe0b3ae4b02da806778bcfa715f35"
   },
   "source": [
    "Accuracy is 86% so is it good ? I do not know actually, we will see at the end of tutorial.\n",
    "<br> Now the question is why we choose K = 3 or what value we need to choose K. The answer is in model complexity\n",
    "\n",
    "<br> Model complexity:\n",
    "* K has general name. It is called a hyperparameter. For now just know K is hyperparameter and we need to choose it that gives best performace. \n",
    "* Literature says if k is small, model is complex model can lead to overfit. It means that model memorizes the train sets and cannot predict test set with good accuracy.\n",
    "* If k is big, model that is less complex model can lead to underfit. \n",
    "* At below, I range K value from 1 to 25(exclude) and find accuracy for each K value. As you can see in plot, when K is 1 it memozize train sets and cannot give good accuracy on test set (overfit). Also if K is 18, model is lead to underfit. Again accuracy is not enough. However look at when K is 18(best performance), accuracy has highest value almost 88%. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "db2c7062-ce1b-4e8b-9b2f-0ee0cd679a91",
    "_uuid": "18d8739373085a9964071f38b8f2adcb64f25491"
   },
   "outputs": [],
   "source": [
    "# Model complexity\n",
    "neig = np.arange(1, 25)\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "# Loop over different values of k\n",
    "for i, k in enumerate(neig):\n",
    "    # k from 1 to 25(exclude)\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    # Fit with knn\n",
    "    knn.fit(x_train,y_train)\n",
    "    #train accuracy\n",
    "    train_accuracy.append(knn.score(x_train, y_train))\n",
    "    # test accuracy\n",
    "    test_accuracy.append(knn.score(x_test, y_test))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=[13,8])\n",
    "plt.plot(neig, test_accuracy, label = 'Testing Accuracy')\n",
    "plt.plot(neig, train_accuracy, label = 'Training Accuracy')\n",
    "plt.legend()\n",
    "plt.title('K-value VS Accuracy')\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(neig)\n",
    "plt.savefig('graph.png')\n",
    "plt.show()\n",
    "print(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b598ee81-e535-49c0-b53c-b13b0a5058db",
    "_uuid": "96423b4f710966c8071647874623c139c1c79bb7"
   },
   "source": [
    "### Up to this point what you learn:\n",
    "* Supervised learning\n",
    "* Exploratory data analysis\n",
    "* KNN\n",
    "    * How to split data\n",
    "    * How to fit, predict data\n",
    "    * How to measure medel performance (accuracy)\n",
    "    * How to choose hyperparameter (K)\n",
    "    \n",
    "**<br> What happens if I chance the title KNN and make it some other classification technique like Random Forest?**\n",
    "* The answer is **nothing**. What you need to is just watch a video about what is random forest in youtube and implement what you learn in KNN. Because the idea and even most of the codes (only KNeighborsClassifier need to be RandomForestClassifier ) are same. You need to split, fit, predict your data and measue performance and choose hyperparameter of random forest(like max_depth). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f9d427a9-faa5-46cf-9e3c-2c8cea2571ad",
    "_uuid": "d075fd2a7c05e5414e33b7b1314d81a6b945e7b3"
   },
   "source": [
    "<a id=\"5\"></a> <br>\n",
    "### REGRESSION\n",
    "* Supervised learning\n",
    "* We will learn linear and logistic regressions\n",
    "* This orthopedic patients data is not proper for regression so I only use two features that are *sacral_slope* and *pelvic_incidence* of abnormal \n",
    "    * I  consider feature is pelvic_incidence and target is sacral_slope \n",
    "    * Lets look at scatter plot so as to understand it better\n",
    "    * reshape(-1,1): If you do not use it shape of x or y becaomes (210,) and we cannot use it in sklearn, so we use shape(-1,1) and shape of x or y be (210, 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6b072c42-059f-4e45-9cfa-8ed39b274f72",
    "_uuid": "d2655c140423b1228c42d2e8dfe54344ba43dcb0"
   },
   "outputs": [],
   "source": [
    "# create data1 that includes pelvic_incidence that is feature and sacral_slope that is target variable\n",
    "data1 = data[data['class'] =='Abnormal']\n",
    "x = np.array(data1.loc[:,'pelvic_incidence']).reshape(-1,1)\n",
    "y = np.array(data1.loc[:,'sacral_slope']).reshape(-1,1)\n",
    "# Scatter\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.scatter(x=x,y=y)\n",
    "plt.xlabel('pelvic_incidence')\n",
    "plt.ylabel('sacral_slope')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "874ac5e0-5bc0-4429-b6c5-707690b5dd77",
    "_uuid": "c84719d363ff736e96a0a575dfd0381bcbc549fb"
   },
   "source": [
    "Now we have our data to make regression. In regression problems target value is continuously varying variable such as price of house or sacral_slope. Lets fit line into this points.\n",
    "\n",
    "<br> Linear regression\n",
    "* y = ax + b       where  y = target, x = feature and a = parameter of model\n",
    "* We choose parameter of model(a) according to minimum error function that is lost function\n",
    "* In linear regression we use Ordinary Least Square (OLS) as lost function.\n",
    "* OLS: sum all residuals but some positive and negative residuals can cancel each other so we sum of square of residuals. It is called OLS\n",
    "* Score: Score uses R^2 method that is ((y_pred - y_mean)^2 )/(y_actual - y_mean)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fb7991f3-5869-4df0-bf6c-30f61e8215c6",
    "_uuid": "7cdc74efa8c46dab5f14f6cc2779928c11a4fa62"
   },
   "outputs": [],
   "source": [
    "# LinearRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "# Predict space\n",
    "predict_space = np.linspace(min(x), max(x)).reshape(-1,1)\n",
    "# Fit\n",
    "reg.fit(x,y)\n",
    "# Predict\n",
    "predicted = reg.predict(predict_space)\n",
    "# R^2 \n",
    "print('R^2 score: ',reg.score(x, y))\n",
    "# Plot regression line and scatter\n",
    "plt.plot(predict_space, predicted, color='black', linewidth=3)\n",
    "plt.scatter(x=x,y=y)\n",
    "plt.xlabel('pelvic_incidence')\n",
    "plt.ylabel('sacral_slope')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "242dd057-bb8a-463a-b04c-1072d5de1e0a",
    "_uuid": "3cf254d2feef8f9f886b0e1de12ec2b155083d2b"
   },
   "source": [
    "<a id=\"6\"></a> <br>\n",
    "### CROSS VALIDATION\n",
    "\n",
    "\n",
    "\n",
    "\\*[Why Train Validation Test Data](https://medium.com/datadriveninvestor/data-science-essentials-why-train-validation-test-data-b7f7d472dc1f)\n",
    "\n",
    "> The primary objective of test data is to give an unbiased estimate of model accuracy. It should be used at the very end and only for a couple of times. If you tune your model after looking at the test accuracies, you are technically leaking information and hence cheating.\n",
    "\n",
    "<br> Cross Validation (CV)\n",
    "* K folds = K fold CV.\n",
    "* Look at this image it defines better than me :)\n",
    "* When K is increase, computationally cost is increase\n",
    "* cross_val_score(reg,x,y,cv=5): use reg(linear regression) with x and y that we define at above and K is 5. It means 5 times(split, train,predict)\n",
    "\n",
    "![img](https://i.stack.imgur.com/1fXzJ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bf504792-66f4-411e-bc1d-5b049da959ac",
    "_uuid": "1f1a77c5d5e6ca52c0264875362665f228e66078"
   },
   "outputs": [],
   "source": [
    "# CV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "reg = LinearRegression()\n",
    "k = 5\n",
    "cv_result = cross_val_score(reg,x,y,cv=k) # uses R^2 as score \n",
    "print('CV Scores: ',cv_result)\n",
    "print('CV scores average: ',np.sum(cv_result)/k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "27ffec30-4951-4479-9d86-32495a80c08d",
    "_uuid": "be425f142c0acc1ab3009fafafa3616948f5c8a5"
   },
   "source": [
    "### Regularized Regression\n",
    "[Regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics)) is a technique used in machine learning to mitigate the problem of [overfitting](https://en.wikipedia.org/wiki/Overfitting).\n",
    "\n",
    "Overfitting vs. Underfitting\n",
    "![img](https://docs.aws.amazon.com/machine-learning/latest/dg/images/mlconcepts_image5.png)\n",
    "\n",
    "As we learn linear regression choose parameters (coefficients) while minimizing lost function. If linear regression thinks that one of the feature is important, it gives high coefficient to this feature. However, this can cause overfitting that is like memorizing in KNN. In order to avoid overfitting, we use regularization that penalize large coefficients.\n",
    "\n",
    "* Ridge regression: First regularization technique. Also it is called L2 regularization. \n",
    "    * Ridge regression lost fuction = OLS + alpha * sum(parameter^2)\n",
    "    * alpha is parameter we need to choose to fit and predict. Picking alpha is similar to picking K in KNN. As you understand alpha is hyperparameter that we need to choose for best accuracy and model complexity. This process is called hyperparameter tuning.\n",
    "    * What if alpha is zero? lost function = OLS so that is linear rigression :)\n",
    "    * If alpha is small that can cause overfitting\n",
    "    * If alpha is big that can cause underfitting. But do not ask what is small and big. These can be change from problem to problem.\n",
    "* Lasso regression: Second regularization technique. Also it is called L1 regularization. \n",
    "    * Lasso regression lost fuction = OLS + alpha * sum(absolute_value(parameter))\n",
    "    * It can be used to select important features od the data. Because features whose values are not shrinked to zero, is chosen by lasso regression\n",
    "    * In order to choose feature, I add new features in our regression data\n",
    "    \n",
    "<br> Linear vs Ridge vs Lasso\n",
    "First impression: Linear\n",
    "Feature Selection: 1.Lasso 2.Ridge\n",
    "Regression model: 1.Ridge 2.Lasso 3.Linear \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bdc1a379-07a4-4b61-8ac1-d7007ae33783",
    "_uuid": "85fa872e3988295a8fbe752bf96319518ca3595b"
   },
   "outputs": [],
   "source": [
    "# Ridge\n",
    "from sklearn.linear_model import Ridge\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,random_state = 2, test_size = 0.3)\n",
    "ridge = Ridge(alpha = 0.1, normalize = True)\n",
    "ridge.fit(x_train,y_train)\n",
    "ridge_predict = ridge.predict(x_test)\n",
    "print('Ridge score: ',ridge.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "de031d9f-b2f8-4fb1-a305-36cbc4dc970f",
    "_uuid": "57f91f4b4e267bd3eb22adfcdf719778c1901c92"
   },
   "outputs": [],
   "source": [
    "# Lasso\n",
    "from sklearn.linear_model import Lasso\n",
    "x = np.array(data1.loc[:,['pelvic_incidence','pelvic_tilt numeric','lumbar_lordosis_angle','pelvic_radius']])\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,random_state = 3, test_size = 0.3)\n",
    "lasso = Lasso(alpha = 0.1, normalize = True)\n",
    "lasso.fit(x_train,y_train)\n",
    "ridge_predict = lasso.predict(x_test)\n",
    "print('Lasso score: ',lasso.score(x_test,y_test))\n",
    "print('Lasso coefficients: ',lasso.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "84f9d88d-28e9-4b26-8824-ae32de7e143c",
    "_uuid": "d70e2366ccc8797c92d2edacb4ab3b59fad4506d"
   },
   "source": [
    "As you can see *pelvic_incidence* and *pelvic_tilt numeric* are important features but others are not important\n",
    "\n",
    "<br> Now lets discuss accuracy. Is it enough for measurement of model selection. For example, there is a data that includes 95% normal and 5% abnormal samples and our model uses accuracy for measurement metric. Then our model predict 100% normal for all samples and accuracy is 95% but it classify all abnormal samples wrong. Therefore we use [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) as a model performance measurement.\n",
    "<br> While using confusion matrix lets use Random forest classifier to diversify classification methods.\n",
    "* tp = true positive(20), fp = false positive(7), fn = false negative(8), tn = true negative(58) \n",
    "* tp = Prediction is positive(normal) and actual is positive(normal). \n",
    "* fp = Prediction is positive(normal) and actual is negative(abnormal).\n",
    "* fn = Prediction is negative(abnormal) and actual is positive(normal).\n",
    "* tn = Prediction is negative(abnormal) and actual is negative(abnormal)\n",
    "* precision = tp / (tp+fp)\n",
    "* recall = tp / (tp+fn)\n",
    "* f1 = 2 * precision * recall / ( precision + recall)\n",
    "\n",
    "Confusion matrix example on phishing email prediction:\n",
    "\n",
    "![img](https://www.knime.com/sites/default/files/5-confusion-matrix-class-statistics.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c71a33f5-5784-461b-949d-cb83f23dace6",
    "_uuid": "19fb6fb0f651e249835037a4f4b2f0b4a2619a27"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix with random forest\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "x,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1)\n",
    "rf = RandomForestClassifier(random_state = 4)\n",
    "rf.fit(x_train,y_train)\n",
    "y_pred = rf.predict(x_test)\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print('Confusion matrix: \\n',cm)\n",
    "print('Classification report: \\n',classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fcba81bb-1257-48d0-afe1-a4416237cc73",
    "_uuid": "f2697bbc248102687596713406512b4cb7f24929"
   },
   "outputs": [],
   "source": [
    "# visualize with seaborn library\n",
    "sns.heatmap(cm,annot=True,fmt=\"d\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4552b965-e15d-4e05-9e02-224159e8d508",
    "_uuid": "dbe175099c0c8151c16ad0c78f1414de8fd9ebdc"
   },
   "source": [
    "<a id=\"7\"></a> <br>\n",
    "### ROC Curve with Logistic Regression \n",
    "\n",
    "![img](https://www.knime.com/sites/default/files/5-confusion-matrix-class-statistics.png)\n",
    "\n",
    "* [Receiver operating characteristic curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) -  plotting the recall or true positive rate (TPR, TP / (TP+FN)) against the fall-out or false positive rate (FPR, FP / (FP + TN)) at various threshold settings.\n",
    "* logistic regression output is probabilities\n",
    "* If probability is higher than 0.5 data is labeled 1(abnormal) else 0(normal)\n",
    "* By default logistic regression threshold is 0.5\n",
    "* ROC is receiver operationg characteristic. In this curve x axis is false positive rate and y axis is true positive rate\n",
    "* If the curve in plot is closer to left-top corner, test is more accurate.\n",
    "* ROC curve score is area under curve (AUC) that is computation area under the curve from prediction scores\n",
    "* We want auc to closer 1\n",
    "* fpr = False Positive Rate\n",
    "* tpr = True Positive Rate\n",
    "* If you want, I made ROC, Random forest and K fold CV in this tutorial. https://www.kaggle.com/kanncaa1/roc-curve-with-k-fold-cv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4f4a8f76-9792-485f-83b0-79e2524ab83c",
    "_uuid": "c7fce3a219764388088ec5f3d57ab913f5c05f35"
   },
   "outputs": [],
   "source": [
    "# ROC Curve with logistic regression\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "# abnormal = 1 and normal = 0\n",
    "data['class_binary'] = [1 if i == 'Abnormal' else 0 for i in data.loc[:,'class']]\n",
    "x,y = data.loc[:,(data.columns != 'class') & (data.columns != 'class_binary')], data.loc[:,'class_binary']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=42)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(x_train,y_train)\n",
    "y_pred_prob = logreg.predict_proba(x_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ceb21da5-4ced-400c-a017-60f735802b69",
    "_uuid": "db2a0e5e83aee71a9585ee113cb01170325c96f1"
   },
   "source": [
    "<a id=\"8\"></a> <br>\n",
    "### HYPERPARAMETER TUNING\n",
    "For machine learning models, there are hyperparameters that are need to be tuned\n",
    "* For example: \n",
    "    * k at KNN\n",
    "    * alpha at Ridge and Lasso\n",
    "    * Random forest parameters like max_depth\n",
    "    * linear regression parameters(coefficients)\n",
    "* Hyperparameter tuning: \n",
    "    * try all of combinations of different parameters\n",
    "    * fit all of them\n",
    "    * measure prediction performance\n",
    "    * see how well each performs\n",
    "    * finally choose best hyperparameters\n",
    "* This process is most difficult part of this tutorial. Because we will write a lot of for loops to iterate all combinations. Just I am kidding sorry for this :) (We actually did it at KNN part)\n",
    "* We only need is one line code that is GridSearchCV\n",
    "    * grid: K is from 1 to 50(exclude)\n",
    "    * GridSearchCV takes knn and grid and makes grid search. It means combination of all hyperparameters. Here it is k.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d1a56a4e-a307-4376-8d06-53fc30447ce6",
    "_uuid": "20fb485285b4d27da2e3eb89e8e43797db7c457a"
   },
   "outputs": [],
   "source": [
    "# grid search cross validation with 1 hyperparameter\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = {'n_neighbors': np.arange(1,50)}\n",
    "knn = KNeighborsClassifier()\n",
    "knn_cv = GridSearchCV(knn, grid, cv=3) # GridSearchCV\n",
    "knn_cv.fit(x,y)# Fit\n",
    "\n",
    "# Print hyperparameter\n",
    "print(\"Tuned hyperparameter k: {}\".format(knn_cv.best_params_)) \n",
    "print(\"Best score: {}\".format(knn_cv.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a4401913-229c-4bcc-8dd0-3563b824f6e9",
    "_uuid": "bdc9c267b635db5696e73cb6ba36d093c8f127c5"
   },
   "source": [
    "Other grid search example with 2 hyperparameter\n",
    "* First hyperparameter is C:logistic regression regularization parameter\n",
    "    * If C is high: overfit\n",
    "    * If C is low: underfit\n",
    "* Second hyperparameter is penalty(lost function): l1 (Lasso) or l2(Ridge) as we learnt at linear regression part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logspace(-3, 3, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8988b749-3bf5-448f-830e-edce366348d0",
    "_uuid": "eb5961ae83511410ae491366bd12f70bef9f440b"
   },
   "outputs": [],
   "source": [
    "# grid search cross validation with 2 hyperparameter\n",
    "# 1. hyperparameter is C:logistic regression regularization parameter\n",
    "# 2. penalty l1 or l2\n",
    "# Hyperparameter grid\n",
    "param_grid = {'C': np.logspace(-3, 3, 7), 'penalty': ['l1', 'l2']}\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state = 12)\n",
    "logreg = LogisticRegression()\n",
    "logreg_cv = GridSearchCV(logreg,param_grid,cv=3)\n",
    "logreg_cv.fit(x_train,y_train)\n",
    "\n",
    "# Print the optimal parameters and best score\n",
    "print(\"Tuned hyperparameters : {}\".format(logreg_cv.best_params_))\n",
    "print(\"Best Accuracy: {}\".format(logreg_cv.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "37171c8f-21ad-4628-8279-5cfedb27aafb",
    "_uuid": "7cd0bf0ed62c1858f5de74ee4f85c84765f8bc06"
   },
   "source": [
    "<a id=\"9\"></a> <br>\n",
    "### PRE-PROCESSING DATA\n",
    "* In real life data can include objects or categorical data in order to use them in sklearn we need to encode them into numerical data\n",
    "* In previous dataset, class is *abnormal* and *normal*. Lets convert them into numeric value.\n",
    "* 2 different feature is created with the name *class_Abnormal* and *class_Normal*\n",
    "* However, we will drop one of the column because this is a binary class and we should only need one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "342e6f67-afeb-42b0-9e56-6521fd15e3ad",
    "_uuid": "8168cf770134142252e92022307815e31a931a34"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('../input/column_2C_weka.csv')\n",
    "# get_dummies\n",
    "df = pd.get_dummies(data)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e2541cfc-7d99-45ce-8486-80f7fec79257",
    "_uuid": "032c522e3f423822402c7f75fcfe3818894f68c8"
   },
   "outputs": [],
   "source": [
    "# drop one of the feature\n",
    "df.drop(\"class_Normal\",axis = 1, inplace = True) \n",
    "df.head(10)\n",
    "# instead of two steps we can make it with one step pd.get_dummies(data,drop_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f55c171e-36d4-4873-a0f6-e8ee411d0b64",
    "_uuid": "5465db779fed984dd89ad591889d1dadb88fc056"
   },
   "source": [
    "Other preprocessing step is centering, scaling or normalizing \n",
    "* Some algorithms, such as KNN uses form of distance, which means they're sensitive to the value of the features. For example, when your dataset has a column `age` with values like 31, 23, 46, and etc., and another column `salary` with values like 55000, 70000, 100000, and etc. without normalize or scale the data, distance-based algorithms will be biased because the distances between `salary` is always larger than `age`.\n",
    "\n",
    "\n",
    "* standardization: ( x - x.mean) / x.variance   or   x - x.min / x.range\n",
    "* [pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html): The purpose of the pipeline is to assemble several steps like svm(classifier) and  standardization(pre-processing)\n",
    "* How we create parameters name: for example SVM_ _C :  stepName__parameterName\n",
    "* Then grid search to find best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5d4aef88-f75e-4485-b481-bdeabbc0e4c7",
    "_uuid": "4690eac9c778ba7230fcee5a782a46396aa8c4ef"
   },
   "outputs": [],
   "source": [
    "# SVM, pre-process and pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "steps = [('scalar', StandardScaler()),\n",
    "         ('SVM', SVC())]\n",
    "pipeline = Pipeline(steps)\n",
    "parameters = {'SVM__C':[1, 10, 100],\n",
    "              'SVM__gamma':[0.1, 0.01]}\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state = 1)\n",
    "cv = GridSearchCV(pipeline,param_grid=parameters,cv=3)\n",
    "cv.fit(x_train,y_train)\n",
    "\n",
    "y_pred = cv.predict(x_test)\n",
    "\n",
    "print(\"Accuracy: {}\".format(cv.score(x_test, y_test)))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b343630f-75c8-4362-8cb0-858e71a2f4b0",
    "_uuid": "3a8fede9eadd9bd0438f7cb50c0155491d3f776b",
    "collapsed": true
   },
   "source": [
    "<a id=\"10\"></a> <br>\n",
    "## UNSUPERVISED LEARNING\n",
    "* Unsupervised learning: It uses data that has unlabeled and uncover hidden patterns from unlabeled data. Example, there are orthopedic patients data that do not have labels. You do not know which orthopedic patient is normal or abnormal.\n",
    "* As you know orthopedic patients data is labeled (supervised) data. It has target variables. In order to work on unsupervised learning, lets drop target variables and to visualize just consider *pelvic_radius* and *degree_spondylolisthesis*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e492f7d0-5d05-4717-a149-e046d9435607",
    "_uuid": "f4927a23d072a9416b57f0bd611d71411407a907"
   },
   "source": [
    "<a id=\"11\"></a> <br>\n",
    "### KMEANS\n",
    "[kmeans clustering](https://en.wikipedia.org/wiki/K-means_clustering)\n",
    "* Lets try our first unsupervised method that is KMeans Cluster\n",
    "* KMeans Cluster: The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided. Data points are clustered based on feature similarity\n",
    "* KMeans(n_clusters = 2): n_clusters = 2 means that create 2 cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fa3dce86-b3cd-4a8f-b689-e5bc5f2fe551",
    "_uuid": "53175af6bce4acd6f743250c240033db04921ad4"
   },
   "outputs": [],
   "source": [
    "# As you can see there is no labels in data\n",
    "data = pd.read_csv('../input/column_2C_weka.csv')\n",
    "plt.scatter(data['pelvic_radius'],data['degree_spondylolisthesis'])\n",
    "plt.xlabel('pelvic_radius')\n",
    "plt.ylabel('degree_spondylolisthesis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "63a3245d-0056-46a0-ba65-a1f61d84441f",
    "_uuid": "9550966d8637f2d5c61633d597cd3746fb294e7d"
   },
   "outputs": [],
   "source": [
    "# KMeans Clustering\n",
    "data2 = data.loc[:,['degree_spondylolisthesis','pelvic_radius']]\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters = 2)\n",
    "kmeans.fit(data2)\n",
    "labels = kmeans.predict(data2)\n",
    "plt.scatter(data['pelvic_radius'],data['degree_spondylolisthesis'],c = labels)\n",
    "plt.xlabel('pelvic_radius')\n",
    "plt.xlabel('degree_spondylolisthesis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fc405787-c073-4377-a4d8-0dcb83371888",
    "_uuid": "bb760dd8801ea5d0b7d98d1e22f784a9c999b900"
   },
   "source": [
    "<a id=\"12\"></a> <br>\n",
    "### EVALUATING OF CLUSTERING\n",
    "We cluster data in two groups. Okey well is that correct clustering? In order to evaluate clustering we will use cross tabulation table.\n",
    "* There are two clusters that are *0* and *1* \n",
    "* First class *0* includes 138 abnormal and 100 normal patients\n",
    "* Second class *1* includes 72 abnormal and 0 normal patiens\n",
    "*The majority of two clusters are abnormal patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "df624885-2e0e-474f-82cb-8a498049d8cc",
    "_uuid": "063968754033b4510dcb6f90245a1a9688d4c201"
   },
   "outputs": [],
   "source": [
    "# cross tabulation table\n",
    "df = pd.DataFrame({'labels':labels,\"class\":data['class']})\n",
    "ct = pd.crosstab(df['labels'],df['class'])\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f5c33609-4573-443a-82c5-5dfe9852c582",
    "_uuid": "03df9051e892dc064c706619e796b5c9128c0a68"
   },
   "source": [
    "The new question is that we know how many class data includes, but what if number of class is unknow in data. This is kind of like hyperparameter in KNN or regressions. \n",
    "* inertia: how spread out the clusters are distance from each sample\n",
    "* lower inertia means more clusters\n",
    "* What is the best number of clusters ?\n",
    "    *There are low inertia and not too many cluster trade off so we can choose elbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "710c918b-481b-4f32-94f6-adecd992a97e",
    "_uuid": "6c7e94ea35a4f535857aa2551ed646ff8c28c1ac"
   },
   "outputs": [],
   "source": [
    "# inertia\n",
    "inertia_list = np.empty(8)\n",
    "for i in range(1,8):\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "    kmeans.fit(data2)\n",
    "    inertia_list[i] = kmeans.inertia_\n",
    "plt.plot(range(0,8),inertia_list,'-o')\n",
    "plt.xlabel('Number of cluster')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6449fdb8-7e0b-4432-9ea3-6d866d899704",
    "_uuid": "06031b6c13b5f700bb2e3d53081d990f66c5e11b"
   },
   "source": [
    "<a id=\"13\"></a> <br>\n",
    "### STANDARDIZATION\n",
    "* Standardizaton is important for both supervised and unsupervised learning\n",
    "* Do not forget standardization as pre-processing\n",
    "* As we already have visualized data so you got the idea. Now we can use all features for clustering.\n",
    "* We can use pipeline like supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0ae27970-8319-45a7-ba7c-a8dd9d9b427e",
    "_uuid": "97a7e3a3fa1722970ff34935470be85f2ec7977b"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../input/column_2C_weka.csv')\n",
    "data3 = data.drop('class',axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "65a46d3b-45b5-4bcd-8137-bdd5b6507ab0",
    "_uuid": "6883975e95325db10ebed6703c372cbe4da29a48"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "scalar = StandardScaler()\n",
    "kmeans = KMeans(n_clusters = 2)\n",
    "pipe = make_pipeline(scalar,kmeans)\n",
    "pipe.fit(data3)\n",
    "labels = pipe.predict(data3)\n",
    "df = pd.DataFrame({'labels':labels,\"class\":data['class']})\n",
    "ct = pd.crosstab(df['labels'],df['class'])\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1a31b9e4-f152-4da1-a132-a6e3a8090401",
    "_uuid": "a602f3d3d47f3bcc8b62bf0ed3114a3ca4760a03"
   },
   "source": [
    "<a id=\"14\"></a> <br>\n",
    "### HIERARCHY\n",
    "* vertical lines are clusters\n",
    "* height on dendogram: distance between merging cluster\n",
    "* method= 'single' : closest points of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f04ec5bb-9f74-4036-9dfd-2649a3519d79",
    "_uuid": "a2dc27b105fc443f442b9605237f45ba69d1e1f1"
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage,dendrogram\n",
    "\n",
    "merg = linkage(data3.iloc[200:220,:],method = 'single')\n",
    "dendrogram(merg, leaf_rotation = 90, leaf_font_size = 6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "15b4f98e-0c9e-49de-b69f-f583524c19ac",
    "_uuid": "4166039422a3ed14b4ac779da29401378dfb4dc2"
   },
   "source": [
    "<a id=\"15\"></a> <br>\n",
    "### T - Distributed Stochastic Neighbor Embedding (T - SNE)\n",
    " * learning rate: 50-200 in normal\n",
    " * fit_transform: it is both fit and transform. t-sne has only have fit_transform\n",
    " * Varieties have same position relative to one another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b4ddf817-57ec-45eb-aa03-1ad385cd4f2e",
    "_uuid": "311a16a96b2ef3e9ca3f8e91db48f28837577291"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "model = TSNE(learning_rate=100)\n",
    "transformed = model.fit_transform(data2)\n",
    "x = transformed[:,0]\n",
    "y = transformed[:,1]\n",
    "plt.scatter(x,y,c = color_list )\n",
    "plt.xlabel('pelvic_radius')\n",
    "plt.xlabel('degree_spondylolisthesis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "00555f09-d381-494e-8bb9-7afb768d912a",
    "_uuid": "71d32fd64448612635a015c3a4e41871bddc337d"
   },
   "source": [
    "<a id=\"16\"></a> <br>\n",
    "### PRINCIPLE COMPONENT ANALYSIS (PCA)\n",
    "* Fundemental dimension reduction technique\n",
    "* first step is decorrelation:\n",
    "    * rotates data samples to be aligned with axes\n",
    "    * shifts data asmples so they have mean zero\n",
    "    * no information lost\n",
    "    * fit() : learn how to shift samples\n",
    "    * transform(): apply the learned transformation. It can also be applies test data\n",
    "* Resulting PCA features are not linearly correlated\n",
    "* Principle components: directions of variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "93b827bf-616e-436f-a514-86fafbf44512",
    "_uuid": "04e33157c4c218d873892090649b169babfd03a8"
   },
   "outputs": [],
   "source": [
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "model = PCA()\n",
    "model.fit(data3)\n",
    "transformed = model.transform(data3)\n",
    "print('Principle components: ',model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fc2fdc3e-a5f6-4d67-979e-b9e9cd2e7198",
    "_uuid": "409100f13e8c3e7bea30dab77ad7ea56e2e338e8"
   },
   "outputs": [],
   "source": [
    "# PCA variance\n",
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "pipeline = make_pipeline(scaler,pca)\n",
    "pipeline.fit(data3)\n",
    "\n",
    "plt.bar(range(pca.n_components_), pca.explained_variance_)\n",
    "plt.xlabel('PCA feature')\n",
    "plt.ylabel('variance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3b6f44f2-0add-4f82-a965-859215a9fea9",
    "_uuid": "ec0ad8f2341933207e26f9eaced8a45e16f2bacf"
   },
   "source": [
    "* Second step: intrinsic dimension: number of feature needed to approximate the data essential idea behind dimension reduction\n",
    "* PCA identifies intrinsic dimension when samples have any number of features\n",
    "* intrinsic dimension = number of PCA feature with significant variance\n",
    "* In order to choose intrinsic dimension try all of them and find best accuracy\n",
    "* Also check intuitive way of PCA with this example: https://www.kaggle.com/kanncaa1/tutorial-pca-intuition-and-image-completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fa5ec66c-2338-4298-a6f8-364a0703ca1f",
    "_uuid": "0f1768fd650fe0b85cdf21140c877471c8baf2bc"
   },
   "outputs": [],
   "source": [
    "# apply PCA\n",
    "pca = PCA(n_components = 2)\n",
    "pca.fit(data3)\n",
    "transformed = pca.transform(data3)\n",
    "x = transformed[:,0]\n",
    "y = transformed[:,1]\n",
    "plt.scatter(x,y,c = color_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "41762172-8d1f-47ef-b8ab-79fb3a2b0a19",
    "_uuid": "2c1a42c1de45ce53e04761b4b6c05840fddaee95"
   },
   "source": [
    "# CONCLUSION\n",
    "This is the end of DATA SCIENCE tutorial. The first part is here:\n",
    "<br>  https://www.kaggle.com/kanncaa1/data-sciencetutorial-for-beginners/\n",
    "<br>**If you have any question or suggest, I will be happy to hear it.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
