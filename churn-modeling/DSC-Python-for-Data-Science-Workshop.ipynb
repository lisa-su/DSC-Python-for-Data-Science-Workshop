{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= 'http://www.bigbang-datascience.com/wp-content/uploads/2017/09/cropped-Logo-01.jpg' width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Guide  \n",
    "------------  \n",
    "- [Project Overview](#project-overview)  \n",
    "- [Part 1: Reading Data - Exploratory Data Analysis with Pandas](#I)\n",
    "- [Part 2: Visual data analysis in Python](#II)\n",
    "- [Part 3: Data Pre-processing &  Preparation](#III)\n",
    "- [Part 4: Predictive Analytics](#IV)\n",
    "- [Part 5: Optimization (Hyper Parameter Tuning)](#V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roadmap for Building Machine Learning Models\n",
    "\n",
    "# 1. Prepare Problem\n",
    "# a) Define The Business Objective\n",
    "# b) Select the datasets\n",
    "# c) Load dataset\n",
    "# d) Load libraries\n",
    "\n",
    "\n",
    "# Data Pre-processing\n",
    "# This is the first step in building a machine learning model. Data pre-processing refers to the transformation of data\n",
    "# before feeding it into the model. It deals with the techniques that are used to convert unusable raw data into clean \n",
    "# reliable data.\n",
    "\n",
    "# Since data collection is often not performed in a controlled manner, raw data often contains outliers \n",
    "# (for example, age = 120), nonsensical data combinations (for example, model: bicycle, type: 4-wheeler), missing values, \n",
    "# scale problems, and so on. Because of this, raw data cannot be fed into a machine learning model because it might \n",
    "# compromise the quality of the results. As such, this is the most important step in the process of data science.\n",
    "\n",
    "\n",
    "# 2. Summarize Data\n",
    "# a) Descriptive statistics\n",
    "# b) Data visualizations\n",
    "\n",
    "# 3. Prepare Data\n",
    "# a) Data Cleaning\n",
    "# b) Feature Selection\n",
    "# c) Data Transformation\n",
    "\n",
    "# Model Learning\n",
    "# After pre-processing the data and splitting it into train/test sets (more on this later), we move on to modeling. Models \n",
    "# are nothing but sets of well-defined methods called algorithms that use pre-processed data to learn patterns, which can \n",
    "# later be used to make predictions. There are different types of learning algorithms, including supervised, semi-supervised, \n",
    "# unsupervised, and reinforcement learning. These will be discussed later.\n",
    "\n",
    "# 4. Modeling Strategy\n",
    "# a) Select Suitable Algorithms\n",
    "# b) Select Training/Testing Approaches\n",
    "# c) Train \n",
    "\n",
    "\n",
    "# Model Evaluation\n",
    "# In this stage, the models are evaluated with the help of specific performance metrics. With these metrics, we can go on to \n",
    "# tune the hyperparameters of a model in order to improve it. This process is called hyperparameter optimization. We will \n",
    "# repeat this step until we are satisfied with the performance.\n",
    "\n",
    "# 4. Evaluate Algorithms\n",
    "# a) Split-out validation dataset\n",
    "# b) Test options and evaluation metric\n",
    "# c) Spot Check Algorithms\n",
    "# d) Compare Algorithms\n",
    "\n",
    "# Prediction\n",
    "# Once we are happy with the results from the evaluation step, we will then move on to predictions. Predictions are made \n",
    "# by the trained model when it is exposed to a new dataset. In a business setting, these predictions can be shared with \n",
    "# decision makers to make effective business choices.\n",
    "\n",
    "# 5. Improve Accuracy\n",
    "# a) Algorithm Tuning\n",
    "# b) Ensembles\n",
    "\n",
    "# Model Deployment\n",
    "# The whole process of machine learning does not just stop with model building and prediction. It also involves making use \n",
    "# of the model to build an application with the new data. Depending on the business requirements, the deployment may be a \n",
    "# report, or it may be some repetitive data science steps that are to be executed. After deployment, a model needs proper \n",
    "# management and maintenance at regular intervals to keep it up and running.\n",
    "\n",
    "# 6. Finalize Model\n",
    "# a) Predictions on validation dataset\n",
    "# b) Create standalone model on entire training dataset\n",
    "# c) Save model for later use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"I\"></a>\n",
    "\n",
    "# I.  Reading Data - Exploratory Data Analysis with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article outline\n",
    "1. Demonstration of main Pandas methods\n",
    "2. First attempt on predicting Bank churn\n",
    "3. Useful resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Demonstration of main Pandas methods \n",
    "\n",
    "**[Pandas](http://pandas.pydata.org)** is a Python library that provides extensive means for data analysis. Data scientists often work with data stored in table formats like `.csv`, `.tsv`, or `.xlsx`. Pandas makes it very convenient to load, process, and analyze such tabular data using SQL-like queries. In conjunction with `Matplotlib` and `Seaborn`, `Pandas` provides a wide range of opportunities for visual analysis of tabular data.\n",
    "\n",
    "The main data structures in `Pandas` are implemented with **Series** and **DataFrame** classes. The former is a one-dimensional indexed array of some fixed data type. The latter is a two-dimensional data structure - a table - where each column contains data of the same type. You can see it as a dictionary of `Series` instances. `DataFrames` are great for representing real data: rows correspond to instances (examples, observations, etc.), and columns correspond to features of these instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()  #  Will import Seaborn functionalities\n",
    "# we don't like warnings\n",
    "# you can comment the following 2 lines if you'd like to\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We’ll demonstrate the main methods in action by analyzing a [dataset](https://bigml.com/user/francisco/gallery/dataset/5163ad540c0b5e5b22000383) on the churn rate of telecom operator clients. Let’s read the data (using `read_csv`), and take a look at the first 5 lines using the `head` method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bchurn = pd.read_csv('./churn-modeling/Churn_Modelling.csv')\n",
    "Bchurn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>About printing DataFrames in Jupyter notebooks</summary>\n",
    "<p>\n",
    "In Jupyter notebooks, Pandas DataFrames are printed as these pretty tables seen above while `print(df.head())` looks worse.\n",
    "By default, Pandas displays 20 columns and 60 rows, so, if your DataFrame is bigger, use the `set_option` function as shown in the example below:\n",
    "\n",
    "```python\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "```\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "Recall that each row corresponds to one client, an **instance**, and columns are **features** of this instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will load our dataset in, and perform some rudimentary cleaning.  \n",
    "\n",
    "Do not worry if you do not understand all of the code below. Comments are provided if you are interested in following along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names may be accessed (and changed) using the `.columns` attribute as below\n",
    "print(\"Old Column Names:\\n\", Bchurn.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stripping out spaces from ends of names, and replacing internal spaces with \"_\"\n",
    "print(\"\\nStripping spaces from ends of column names; replacing internal spaces with '_'\\n\")\n",
    "Bchurn.columns = [col.strip().replace(' ', '_').lower() for col in Bchurn.columns]\n",
    "\n",
    "# Print edited column names\n",
    "print(\"\\nNew Column Names:\\n\", Bchurn.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s have a look at data dimensionality, features names, and feature types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Bchurn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, we can see that the table contains 3333 rows and 20 columns.\n",
    "\n",
    "Now let’s try printing out column names using `columns`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Bchurn.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `info()` method to output some general information about the dataframe: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bchurn.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bool`, `int64`, `float64` and `object` are the data types of our features. We see that one feature is logical (`bool`), 3 features are of type `object`, and 16 features are numeric. With this same method, we can easily see if there are any missing values. Here, there are none because each column contains 3333 observations, the same number of rows we saw before with `shape`.\n",
    "\n",
    "We can **change the column type** with the `astype` method. Let’s apply this method to the `Churn` feature to convert it into `int64`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Churn'] = df['Churn'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows\n",
    "len(Bchurn)  # for traceability 576 rows imported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The `describe` method shows basic statistical characteristics of each numerical feature (`int64` and `float64` types): number of non-missing values, mean, standard deviation, range, median, 0.25 and 0.75 quartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bchurn.describe()  # Similar to summary() in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bchurn.describe().transpose()  # change the rows and columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see statistics on non-numerical features, one has to explicitly indicate data types of interest in the `include` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bchurn.describe(include=['object', 'bool'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For categorical (type `object`) and boolean (type `bool`) features we can use the `value_counts` method. Let’s have a look at the distribution of `Churn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bchurn['churned'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2850 users out of 3333 are *loyal*; their `Churn` value is `0`. To calculate fractions, pass `normalize=True` to the `value_counts` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bchurn['churned'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.  Indexing and retrieving data\n",
    "\n",
    "DataFrame can be indexed in different ways. \n",
    "\n",
    "To get a single column, you can use a `DataFrame['Name']` construction. Let's use this to answer a question about that column alone: **what is the proportion of churned users in our dataframe?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bchurn['churned'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "14.5% is actually quite bad for a company; such a churn rate can make the company go bankrupt.\n",
    "\n",
    "**Boolean indexing** with one column is also very convenient. The syntax is `df[P(df['Name'])]`, where `P` is some logical condition that is checked for each element of the `Name` column. The result of such indexing is the DataFrame consisting only of rows that satisfy the `P` condition on the `Name` column. \n",
    "\n",
    "Let’s use it to answer the question:\n",
    "\n",
    "**What are average values of numerical features for churned users?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bchurn[Bchurn['churned'] == 1].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "DataFrames can be indexed by column name (label) or row name (index) or by the serial number of a row. The `loc` method is used for **indexing by name**, while `iloc()` is used for **indexing by number**.\n",
    "\n",
    "In the first case, we would say *\"give us the values of the rows with index from 0 to 5 (inclusive) and columns labeled from State to Area code (inclusive)\"*, and, in the second case, we would say *\"give us the values of the first five rows in the first three columns\"* (as in typical Python slice: the maximal value is not included).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bchurn.loc[0:5, 'geography':'age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bchurn.iloc[0:5, 0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we need the first or the last line of the data frame, we can use the `df[:1]` or `df[-1:]` construct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bchurn[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Applying Functions to Cells, Columns and Rows\n",
    "\n",
    "**To apply functions to each column, use `apply()`:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bchurn.apply(np.max) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"II\"></a>\n",
    "# II. Visual data analysis in Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the field of Machine Learning, *data visualization* is not just making fancy graphics for reports; it is used extensively in day-to-day work for all phases of a project.\n",
    "\n",
    "To start with, visual exploration of data is the first thing one tends to do when dealing with a new task. We do preliminary checks and analysis using graphics and tables to summarize the data and leave out the less important details. It is much more convenient for us, humans, to grasp the main points this way than by reading many lines of raw data. It is amazing how much insight can be gained from seemingly simple charts created with available visualization tools.\n",
    "\n",
    "Next, when we analyze the performance of a model or report results, we also often use charts and images. Sometimes, for interpreting a complex model, we need to project high-dimensional spaces onto more visually intelligible 2D or 3D figures.\n",
    "\n",
    "All in all, visualization is a relatively fast way to learn something new about your data. Thus, it is vital to learn its most useful techniques and make them part of your everyday ML toolbox.\n",
    "\n",
    "We are going to get hands-on experience with visual exploration of data using popular libraries such as `matplotlib` and `seaborn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article outline\n",
    "\n",
    "1. Dataset\n",
    "2. Univariate visualization\n",
    "    * 2.1 Quantitative features\n",
    "    * 2.2 Categorical and binary features\n",
    "3. Multivariate visualization\n",
    "    * 3.1 Quantitative–Quantitative\n",
    "    * 3.2 Quantitative–Categorical\n",
    "    * 3.3 Categorical–Categorical\n",
    "4. Whole dataset\n",
    "    * 4.1 Naive approach\n",
    "    * 4.2 Dimensionality reduction\n",
    "    * 4.2 t-SNE\n",
    "5. Useful resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Univariate visualization\n",
    "\n",
    "*Univariate* analysis looks at one feature at a time. When we analyze a feature independently, we are usually mostly interested in the *distribution of its values* and ignore other features in the dataset.\n",
    "\n",
    "Below, we will consider different statistical types of features and the corresponding tools for their individual visual analysis.\n",
    "\n",
    "#### 1.1 Quantitative features\n",
    "\n",
    "*Quantitative features* take on ordered numerical values. Those values can be *discrete*, like integers, or *continuous*, like real numbers, and usually express a count or a measurement.\n",
    "\n",
    "##### 1.1.1 Histograms and density plots\n",
    "\n",
    "The easiest way to take a look at the distribution of a numerical variable is to plot its *histogram* using the `DataFrame`'s method [`hist()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.hist.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['balance', 'tenure']\n",
    "\n",
    "Bchurn[features].hist(figsize=(10, 4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A histogram groups values into *bins* of equal value range. The shape of the histogram may contain clues about the underlying distribution type: Gaussian, exponential etc. You can also spot any skewness in its shape when the distribution is nearly regular but has some anomalies. Knowing the distribution of the feature values becomes important when you use Machine Learning methods that assume a particular type of it, most often Gaussian.\n",
    "\n",
    "In the above plot, we see that the variable *Total day minutes* is normally distributed, while *Total intl calls* is prominently skewed right (its tail is longer on the right).\n",
    "\n",
    "There is also another, often clearer, way to grasp the distribution: *density plots* or, more formally, *Kernel Density Plots*. They can be considered a [smoothed](https://en.wikipedia.org/wiki/Kernel_smoother) version of the histogram. Their main advantage over the latter is that they do not depend on the size of the bins. Let's create density plots for the same two variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bchurn[features].plot(kind='density', subplots=True, layout=(1, 2), \n",
    "                  sharex=False, figsize=(10, 4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to plot a distribution of observations with `seaborn`'s [`distplot()`](https://seaborn.pydata.org/generated/seaborn.distplot.html). For example, let's look at the distribution of *Total day minutes*. By default, the plot displays both the histogram with the [kernel density estimate](https://en.wikipedia.org/wiki/Kernel_density_estimation) (KDE) on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increasing the width of the Chart\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize'] = 10,6 # similar to par(mfrow = c(2,1), mar = c(4,4,2,1)) # 2 columns and 1 row\n",
    "sns.distplot(Bchurn[\"age\"]) # pass it one variable\n",
    "\n",
    "# if you are getting warnings related to the package you should use ignore function\n",
    "import warnings\n",
    "warnings.filterwarnings ('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# increasing the width of the Chart\n",
    "plt.rcParams['figure.figsize'] = 10,6 # similar to par(mfrow = c(2,1), mar = c(4,4,2,1)) # 2 columns and 1 row\n",
    "sns.distplot(Bchurn[\"estimatedsalary\"]) # pass it one variable\n",
    "\n",
    "# if you are getting warnings related to the package you should use ignore function\n",
    "import warnings\n",
    "warnings.filterwarnings ('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Categorical and binary features\n",
    "\n",
    "*Categorical features* take on a fixed number of values. Each of these values assigns an observation to a corresponding group, known as a *category*, which reflects some qualitative property of this example. *Binary* variables are an important special case of categorical variables when the number of possible values is exactly 2. If the values of a categorical variable are ordered, it is called *ordinal*.\n",
    "\n",
    "##### 1.2.1 Frequency table\n",
    "\n",
    "Let’s check the class balance in our dataset by looking at the distribution of the target variable: the *churn rate*. First, we will get a frequency table, which shows how frequent each value of the categorical variable is. For this, we will use the [`value_counts()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bchurn['churned'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the entries in the output are sorted from the most to the least frequently-occurring values.\n",
    "\n",
    "In our case, the data is not *balanced*; that is, our two target classes, loyal and disloyal customers, are not represented equally in the dataset. Only a small part of the clients canceled their subscription to the telecom service. As we will see in the following articles, this fact may imply some restrictions on measuring the classification performance, and, in the future, we may want to additionaly penalize our model errors in predicting the minority \"Churn\" class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2 Bar plot\n",
    "\n",
    "The bar plot is a graphical representation of the frequency table. The easiest way to create it is to use the `seaborn`'s function [`countplot()`](https://seaborn.pydata.org/generated/seaborn.countplot.html). There is another function in `seaborn` that is somewhat confusingly called [`barplot()`](https://seaborn.pydata.org/generated/seaborn.barplot.html) and is mostly used for representation of some basic statistics of a numerical variable grouped by a categorical feature.\n",
    "\n",
    "Let's plot the distributions for two categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "\n",
    "sns.countplot(x='churned', data=Bchurn, ax=axes[0]);\n",
    "sns.countplot(x='age', data=Bchurn, ax=axes[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3. Distributions of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributions of categorical features\n",
    "plt.rcParams['figure.figsize'] = 8,6\n",
    "sns.countplot(y='gender', data=Bchurn)\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(y='geography', data=Bchurn)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the histograms, discussed above, and bar plots may look similar, there are several differences between them:\n",
    "1. *Histograms* are best suited for looking at the distribution of numerical variables while *bar plots* are used for categorical features.\n",
    "2. The values on the X-axis in the *histogram* are numerical; a *bar plot* can have any type of values on the X-axis: numbers, strings, booleans.\n",
    "3. The *histogram*'s X-axis is a *Cartesian coordinate axis* along which values cannot be changed; the ordering of the *bars* is not predefined. Still, it is useful to note that the bars are often sorted by height, that is, the frequency of the values. Also, when we consider *ordinal* variables (like *Customer service calls* in our data), the bars are usually ordered by variable value.\n",
    "\n",
    "The left chart above vividly illustrates the imbalance in our target variable. The bar plot for *Customer service calls* on the right gives a hint that the majority of customers resolve their problems in maximum 2–3 calls. But, as we want to be able to predict the minority class, we may be more interested in how the fewer dissatisfied customers behave. It may well be that the tail of that bar plot contains most of our churn. These are just hypotheses for now, so let's move on to some more interesting and powerful visual techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multivariate visualization\n",
    "\n",
    "*Multivariate* plots allow us to see relationships between two and more different variables, all in one figure. Just as in the case of univariate plots, the specific type of visualization will depend on the types of the variables being analyzed.\n",
    "\n",
    "#### 2.1 Quantitative–Quantitative\n",
    "\n",
    "##### 2.1.1 Correlation matrix\n",
    "\n",
    "Let's look at the correlations among the numerical variables in our dataset. This information is important to know as there are Machine Learning algorithms (for example, linear and logistic regression) that do not handle highly correlated input variables well.\n",
    "\n",
    "First, we will use the method [`corr()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.corr.html) on a `DataFrame` that calculates the correlation between each pair of features. Then, we pass the resulting *correlation matrix* to [`heatmap()`](https://seaborn.pydata.org/generated/seaborn.heatmap.html) from `seaborn`, which renders a color-coded matrix for the provided values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bchurn.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-numerical variables\n",
    "numerical = list(set(Bchurn.columns) - \n",
    "                 set(['rowNumber', 'customerId', 'gender', 'surname','geography']))\n",
    "\n",
    "# Calculate Correlation\n",
    "corr_matrix = Bchurn[numerical].corr()\n",
    "corr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 4.2 Correlation heatmap of the numberic variables\n",
    "plt.rcParams['figure.figsize'] = 20,10  # control plot sizeimport seaborn as sns\n",
    "sns.heatmap(Bchurn.corr(), cmap = \"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaborn\n",
    "## first_twenty = har_train.iloc[:, :20] # pull out first 20 feats\n",
    "corr = Bchurn.corr()  # compute correlation matrix\n",
    "mask = np.zeros_like(corr, dtype=np.bool)  # make mask\n",
    "mask[np.triu_indices_from(mask)] = True  # mask the upper triangle\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 9))  # create a figure and a subplot\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)  # custom color map\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    mask=mask,\n",
    "    cmap=cmap,\n",
    "    center=0,\n",
    "    linewidth=0.5,\n",
    "    cbar_kws={'shrink': 0.5}\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box and Whisker Plots\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = 20,10  # control plot size\n",
    "Bchurn.plot(kind='box', subplots=True, layout=(3,6), sharex=False, sharey=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Density Plots\n",
    "plt.rcParams['figure.figsize'] = 20,10  # control plot size\n",
    "\n",
    "Bchurn.plot(kind='density', subplots=True, layout=(4,4), sharex=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Histograms\n",
    "plt.rcParams['figure.figsize'] = 20,10  # control plot size\n",
    "\n",
    "Bchurn.hist()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.2 Scatter plot\n",
    "\n",
    "The *scatter plot* displays values of two numerical variables as *Cartesian coordinates* in 2D space. Scatter plots in 3D are also possible.\n",
    "\n",
    "Let's try out the function [`scatter()`](https://matplotlib.org/devdocs/api/_as_gen/matplotlib.pyplot.scatter.html) from the `matplotlib` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Bchurn['balance'], Bchurn['estimatedsalary']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an uninteresting picture of two normally distributed variables. Also, it seems that these features are uncorrelated because the ellpise-like shape is aligned with the axes.\n",
    "\n",
    "There is a slightly fancier option to create a scatter plot with the `seaborn` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='balance', y='estimatedsalary', \n",
    "              data=Bchurn, kind='scatter');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function [`jointplot()`](https://seaborn.pydata.org/generated/seaborn.jointplot.html) plots two histograms that may be useful in some cases.\n",
    "\n",
    "Using the same function, we can also get a smoothed version of our bivariate distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot('tenure', 'age', data=Bchurn,\n",
    "              kind=\"kde\", color=\"g\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is basically a bivariate version of the *Kernel Density Plot* discussed earlier.\n",
    "\n",
    "##### 2.1.3 Scatterplot matrix\n",
    "\n",
    "In some cases, we may want to plot a *scatterplot matrix* such as the one shown below. Its diagonal contains the distributions of the corresponding variables, and the scatter plots for each pair of variables fill the rest of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `pairplot()` may become very slow with the SVG format\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "sns.pairplot(Bchurn[numerical]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"III\"></a>\n",
    "# III- Data Pre-processing &  Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bchurn = pd.read_csv('Churn_Modelling.csv')\n",
    "Bchurn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data types\n",
    "\n",
    "print(Bchurn.dtypes)\n",
    "# Bchurn = Bchurn.astype(float)\n",
    "# print(Bchurn.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude unwanted columns\n",
    "Bchurn = Bchurn[['CreditScore', 'Geography','Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n",
    "       'IsActiveMember', 'EstimatedSalary', 'Churned']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skewness along the index axis \n",
    "Bchurn.skew(axis = 0, skipna = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Use merge function to conbine two files\n",
    "\n",
    "df = pd.merge(df1, df2, on = 'Student_id')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2. Fixing missing values\n",
    "\n",
    "#### 2.1 Delete a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding missing values\n",
    "\n",
    "Bchurn.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Delete rows with NaN values\n",
    "\n",
    "Once you have figured out all the missing details, we remove all the missing rows from the DataFrame. To do so, we use the dropna() function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing Null values\n",
    "\n",
    "Bchurn = Bchurn.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete a column\n",
    "'''Bchurn.drop('Code', axis=1, inplace=True)\n",
    "print(Bchurn.shape)\n",
    "Bchurn.head(10)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete rows with NaN values\n",
    "\n",
    "'''print(Bchurn.shape)\n",
    "Bchurn[['Bare-Nuclei']] = Bchurn[['Bare-Nuclei']].replace('?', numpy.NaN)\n",
    "Bchurn.dropna(axis=0, how='any', inplace=True)\n",
    "print(data2.shape)'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Mark value as NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark value as NaN\n",
    "\n",
    "'''print(pd.unique(Bchurn['Bare-Nuclei']))\n",
    "Bchurn[['Bare-Nuclei']] = Bchurn[['Bare-Nuclei']].replace('?', numpy.NaN)\n",
    "print(pandas.unique(dataset['Bare-Nuclei']))'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Impute missing values \n",
    "\n",
    "Impute the numerical data of the age column with its mean. To do so, first find the mean of the column with missing values using the mean() function of pandas, and then print it  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute numerical data with mean '\n",
    "mean_age = Bchurn.Age.mean()\n",
    "Bchurn.Age.fillna(mean_age, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We couild also impute the categorical data  with its mode. We first need to find the mode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute Categorical data with mode\n",
    "mode_Gender = Bchurn.Gender.mode()[0]\n",
    "print(mode_Gender)\n",
    "\n",
    "# Impute the missing data of the contact column with its mode using the fillna() function\n",
    "Bchurn.Gender.fillna(mode_Gender,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.5 Splitting data - on whether or not \"Age\" is specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''### Splitting data - on whether or not \"Age\" is specified.\n",
    "\n",
    "# Training data -- \"Age\" Not null; \"Age\" as target\n",
    "train = titanic_knn[titanic_knn.Age.notnull()]\n",
    "X_train = train.drop(['Age'], axis = 1)\n",
    "y_train = train.Age'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.5.1 Data to impute, -- Where Age is null; Remove completely-null \"Age\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Data to impute, -- Where Age is null; Remove completely-null \"Age\" column.\n",
    "impute = titanic_knn[titanic_knn.Age.isnull()].drop(['Age'], axis = 1)\n",
    "print(\"Data to Impute\")\n",
    "print(impute.head(3))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.5.2 import algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# import algorithm\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Instantiate\n",
    "knr = KNeighborsRegressor()\n",
    "\n",
    "# Fit\n",
    "knr.fit(X_train, y_train)\n",
    "\n",
    "# Create Predictions\n",
    "imputed_ages = knr.predict(impute)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.5.3 Add to Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Add to Df\n",
    "impute['Age'] = imputed_ages\n",
    "print(\"\\nImputed Ages\")\n",
    "print(impute.head(3))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.5.4 Re-combine dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Re-combine dataframes\n",
    "titanic_imputed = pd.concat([train, impute], sort = False, axis = 0)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.5.5 Return to original order - to match back up with \"Survived\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Return to original order - to match back up with \"Survived\"\n",
    "titanic_imputed.sort_index(inplace = True)\n",
    "print(\"Shape with imputed values:\", titanic_imputed.shape)\n",
    "print(\"Shape before imputation:\", titanic_knn.shape)\n",
    "titanic_imputed.head(7)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. Finding and Fixing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Boxplots\n",
    "# Horizontal boxplot with observations\n",
    "plt.rcParams['figure.figsize'] = 8,4\n",
    "sns.boxplot(Bchurn['Age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The height of the histogram bars here is normed and shows the density rather than the number of examples in each bin.\n",
    "\n",
    "##### Box plot\n",
    "\n",
    "Another useful type of visualization is a *box plot*. `seaborn` does a great job here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 4,8\n",
    "sns.boxplot(x='Age', data=Bchurn, orient = 'v');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplot uses the IQR method to display the data and the outliers (the shape of the data). But in order to print an outlier, we use a mathematical formula to retrieve it. Add the following code to find the outliers of the Age column using the IQR method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = Bchurn[\"Age\"].quantile(0.25)\n",
    "\n",
    "Q3 = Bchurn[\"Age\"].quantile(0.75)\n",
    "\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "print(IQR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we find the upper fence and lower fence by adding the following code, and print all the data above the upper fence and below the lower fence. Add the following code to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lower_Fence = Q1 - (1.5 * IQR)\n",
    "\n",
    "Upper_Fence = Q3 + (1.5 * IQR)\n",
    "\n",
    "print(Lower_Fence)\n",
    "\n",
    "print(Upper_Fence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out the outlier data and print only the potential data. To do so, just negate the preceding result using the ~ operator:\n",
    "Bchurn[((Bchurn[\"Age\"] < Lower_Fence) |(Bchurn[\"Age\"] > Upper_Fence))].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out the outlier data and print only the potential data. To do so, just negate the preceding result using the ~ operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bchurn = Bchurn[~((Bchurn [\"Age\"] < Lower_Fence) |(Bchurn[\"Age\"] > Upper_Fence))]\n",
    "Bchurn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if outliers are fixed\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 4,8\n",
    "sns.boxplot(x='Age', data=Bchurn, orient = 'v');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Standardize data (0 mean, 1 stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Feature Importance with Extra Trees Classifier\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "array = Bchurn.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# feature extraction\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, Y)\n",
    "print(model.feature_importances_)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Identify Features with Low Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Identify Features with Low Variance\n",
    "from pandas import read_csv\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "# load data\n",
    "array = Bchurn.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "# feature selection\n",
    "threshold = 0.8 * (1 - 0.8)\n",
    "test = VarianceThreshold(threshold)\n",
    "fit = test.fit(X)\n",
    "print(fit.variances_)\n",
    "features = fit.transform(X)\n",
    "print(features)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Feature Extraction with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Feature Extraction with PCA\n",
    "import numpy\n",
    "from pandas import read_csv\n",
    "from sklearn.decomposition import PCA\n",
    "# load data\n",
    "array = Bchurn.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "# feature extraction\n",
    "pca = PCA(n_components=3)\n",
    "fit = pca.fit(X)\n",
    "# summarize components\n",
    "print(\"Explained Variance: %s\") % fit.explained_variance_ratio_\n",
    "print(fit.components_)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Feature Extraction with Recursive Feature Elimination (REF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Feature Extraction with RFE\n",
    "from pandas import read_csv\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# load data\n",
    "array = Bchurn.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "# feature extraction\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, 3)\n",
    "fit = rfe.fit(X, Y)\n",
    "print ()\"Num Features: %d\" % fit.n_features_\n",
    "print(\"Selected Features: %s\") % fit.support_)\n",
    "print(\"Feature Ranking: %s\") % fit.ranking_\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Feature Extraction with Univariate Statistical Tests (Chi-squared for classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Feature Extraction with Univariate Statistical Tests (Chi-squared for classification)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "# load data\n",
    "array = Bchurn.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "# feature extraction\n",
    "test = SelectKBest(score_func=chi2, k=4)\n",
    "fit = test.fit(X, Y)\n",
    "# summarize scores\n",
    "numpy.set_printoptions(precision=3)\n",
    "print(fit.scores_)\n",
    "features = fit.transform(X)\n",
    "# summarize selected features\n",
    "print(features[0:5,:])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Transformation\n",
    "\n",
    "There are some algorithms that can work well with categorical data, such as decision trees. But most machine learning algorithms cannot operate directly with categorical data. These algorithms require the input and output both to be in numerical form. If the output to be predicted is categorical, then after prediction we convert them back to categorical data from numerical data. Let's discuss some key challenges that we face while dealing with categorical data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Simple Replacement of Categorical Data with a Number\n",
    "\n",
    "Find the categorical column and separate it out with a different dataframe. To do so, use the select_dtypes() function from pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bchurn_categorical = Bchurn.select_dtypes(exclude=[np.number])\n",
    "\n",
    "Bchurn_categorical.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also extract the categorial featuers using boolean mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical boolean mask\n",
    "categorical_feature_mask = Bchurn.dtypes==object\n",
    "# filter categorical columns using mask and turn it into a list\n",
    "categorical_cols = Bchurn.columns[categorical_feature_mask].tolist()\n",
    "\n",
    "categorical_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the distinct unique values in the Geography and Gender column. To do so, use the unique() function from pandas with the column name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Bchurn_categorical['Geography'].unique())\n",
    "print(Bchurn_categorical['Gender'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bchurn_categorical.Gender.replace({\"Female\":1, \"Male\":2, \"3rd Class\":3}, inplace= True)\n",
    "Bchurn_categorical.Geography.replace({\"France\":1, \"Spain\":2, \"Germany\":3}, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bchurn_categorical.Gender.head()\n",
    "Bchurn_categorical.Geography.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Label Encoding\n",
    "\n",
    "This is a technique in which we replace each value in a categorical column with numbers from 0 to N-1. For example, say we've got a list of employee names in a column. After performing label encoding, each employee name will be assigned a numeric label. But this might not be suitable for all cases because the model might consider numeric values to be weights assigned to the data. Label encoding is the **best method to use for ordinal data**. The scikit-learn library provides LabelEncoder(), which helps with label encoding. Let's look at an exercise in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing the encoding, remove all the missing data. To do so, use the dropna() function, Select all the columns that are not numeric using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_column_category = Bchurn.select_dtypes(exclude=[np.number]).columns\n",
    "data_column_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through this category column and convert it to numeric data using LabelEncoder(). To do so, import the sklearn.preprocessing package and use the LabelEncoder() class to transform the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the LabelEncoder class\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Creating the object instance\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for i in data_column_category:\n",
    "\n",
    "    Bchurn[i] = label_encoder.fit_transform(Bchurn[i])\n",
    "\n",
    "print(\"Label Encoded Data: \")\n",
    "\n",
    "Bchurn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 One-Hot Encoding\n",
    "\n",
    "In label encoding, categorical data is converted to numerical data, and the values are assigned labels (such as 1, 2, and 3). Predictive models that use this numerical data for analysis might sometimes mistake these labels for some kind of order (for example, a model might think that a label of 3 is \"better\" than a label of 1, which is incorrect). In order to avoid this confusion, we can use one-hot encoding. Here, the label-encoded data is further divided into n number of columns. Here, n denotes the total number of unique labels generated while performing label encoding. For example, say that three new labels are generated through label encoding. Then, while performing one-hot encoding, the columns will be divided into three parts. So, the value of n is 3. Let's look at an exercise to get further clarification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have performed label encoding, we execute one-hot encoding. Add the following code to implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing Onehot Encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "onehot_encoded = onehot_encoder.fit_transform(Bchurn[data_column_category])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a new dataframe with the encoded data and print the first five rows. Add the following code to do this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoded_frame = pd.DataFrame(onehot_encoded, columns = onehot_encoder.get_feature_names(data_column_category))\n",
    "\n",
    "onehot_encoded_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every level or category, a new column is created. In order to prefix the category name with the column name you can use this alternate way to create one-hot encoding. In order to prefix the category name with the column name, write the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4  Dummy Varaibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables for the categorical features\n",
    "to_dummy = ['Gender','Geography']\n",
    "Bchurn_getdummies  = pd.get_dummies(Bchurn, prefix = to_dummy, columns = to_dummy, drop_first = True)\n",
    "Bchurn_getdummies .head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data discretization is the process of converting continuous data into discrete buckets by grouping it. Discretization is also known for easy maintainability of the data. Training a model with discrete data becomes faster and more effective than when attempting the same with continuous data. Although continuous-valued data contains more information, huge amounts of data can slow the model down. Here, discretization can help us strike a balance between both. Some famous methods of data discretization are binning and using a histogram. Although data discretization is useful, we need to effectively pick the range of each bucket, which is a challenge. \n",
    "\n",
    "The main challenge in discretization is to choose the number of intervals or bins and how to decide on their width.\n",
    "\n",
    "Here we make use of a function called pandas.cut(). This function is useful to achieve the bucketing and sorting of segmented data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bchurn['Tenure'] = pd.cut(Bchurn['Tenure'],5,labels=['Poor','Below_average','Average','Above_Average','Excellent'])\n",
    "\n",
    "Bchurn.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# binarization\n",
    "from sklearn.preprocessing import Binarizer\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "array = Bchurn.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "binarizer = Binarizer(threshold=0.0).fit(X)\n",
    "binaryX = binarizer.transform(X)\n",
    "# summarize transformed data\n",
    "numpy.set_printoptions(precision=3)\n",
    "print(binaryX[0:5,:])'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Box-Cox transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Box-Cox transform\n",
    "import pandas\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "array = Bchurn.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "X_boxcox = boxcox(1+X[:,2])[0]\n",
    "\n",
    "print(X_boxcox)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Convert a string class label to an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Convert a string class label to an integer\n",
    "import pandas\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "array = Bchurn.values\n",
    "\n",
    "y = array[:, 60]\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "print(encoder.classes_)\n",
    "encoded_y = encoder.transform(y)\n",
    "print(encoded_y)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Normalize data (length of 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''# Normalize data (length of 1)\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import pandas\n",
    "import numpy\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "# separate array into input and output components\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "scaler = Normalizer().fit(X)\n",
    "normalizedX = scaler.transform(X)\n",
    "# summarize transformed data\n",
    "numpy.set_printoptions(precision=3)\n",
    "print(normalizedX[0:5,:])\n",
    "''''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real life, values in a dataset might have a variety of different magnitudes, ranges, or scales. Algorithms that use distance as a parameter may not weigh all these in the same way. There are various data transformation techniques that are used to transform the features of our data so that they use the same scale, magnitude, or range. This ensures that each feature has an appropriate effect on a model's predictions.\n",
    "\n",
    "Some features in our data might have high-magnitude values (for example, annual salary), while others might have relatively low values (for example, the number of years worked at a company). Just because some data has smaller values does not mean it is less significant. So, to make sure our prediction does not vary because of different magnitudes of features in our data, we can perform feature scaling, standardization, or normalization (these are three similar ways of dealing with magnitude issues in data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Rescale data (between 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Rescale data (between 0 and 1)\n",
    "import pandas\n",
    "import scipy\n",
    "import numpy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "# separate array into input and output components\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "# summarize transformed data\n",
    "numpy.set_printoptions(precision=3)\n",
    "print(rescaledX[0:5,:])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Standardize data (0 mean, 1 stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Standardize data (0 mean, 1 stdev)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas\n",
    "import numpy\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "# separate array into input and output components\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "scaler = StandardScaler().fit(X)\n",
    "rescaledX = scaler.transform(X)\n",
    "# summarize transformed data\n",
    "numpy.set_printoptions(precision=3)\n",
    "print(rescaledX[0:5,:])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"IV\"></a>\n",
    "## IV Predictive Analytics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Identifying X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bchurn = pd.read_csv('Churn_Modelling.csv')\n",
    "Bchurn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bchurn.columns\n",
    "Bchurn = Bchurn[[ 'CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n",
    "       'IsActiveMember', 'EstimatedSalary', 'Churned']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''X = Bchurn.iloc[:, [0,7]].values\n",
    "y = Bchurn.iloc[:, 8].values'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Bchurn.drop('Churned', axis = 1)\n",
    "y = Bchurn.Churned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''X = Bchurn.iloc[:, [3,6,7,8,8,10,11,12,13]].values\n",
    "y = Bchurn.iloc[:, 13].values'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Data split & Scaling Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "'''from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Building Model (Decision Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.1 First Method (using function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier as Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(features, target):\n",
    "#     model = Model()\n",
    "#     model.fit(features, target)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict(model, new_features):\n",
    "#     preds = model.predict(new_features)\n",
    "#     return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assume Titanic data is loaded into titanic_feats,\n",
    "# # titanic_target and titanic_test\n",
    "# model = train(X_train, y_train)\n",
    "# predictions = predict(model, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.2 Second Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Decision Tree Classification to the Training set\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTreeModel = DecisionTreeClassifier(criterion = 'entropy', random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTreeModel.fit(X_train, y_train)  # Training input and its Target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTreePred = DecisionTreeModel.predict(X_test) # I already Know y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  Making the Confusion Matrix\n",
    "**Accuracy** is perhaps the most intuitive performance measure. It is simply the ratio of correctly predicted observations.  \n",
    "**Precision**: Precision looks at the ratio of correct positive observations   \n",
    "**Recall** : Recall is also known as sensitivity or true positive rate. It is the ratio of correctly predicted positive events   \n",
    "**F1 Score** : The F1 Score is the weighted average of Precision and recall. Therefore, this score takes both false postives and false negatives into account   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import machine learning modules\n",
    "from sklearn.ensemble import GradientBoostingClassifier, partial_dependence\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "CMTD = confusion_matrix(y_test,DecisionTreePred ) # Compare the predicted target varaible to the orginal target variable\n",
    "CMTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Churned'\n",
    "CMTD = pd.crosstab(y_test,DecisionTreePred, rownames=['Actual'], colnames=['Predicted'])\n",
    "fig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\n",
    "sns.heatmap(CMTD, \n",
    "            xticklabels=['Active', 'Churned'],\n",
    "            yticklabels=['Active', 'Churned'],\n",
    "            annot=True,ax=ax1,\n",
    "            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\n",
    "plt.title('Confusion Matrix', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Score\n",
    "ACDT = accuracy_score(y_test, DecisionTreePred )\n",
    "print(ACDT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3  Plot feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "plt.title('Normalized Feature Importances')\n",
    "sns.barplot(y=X.columns, x=DecisionTreeModel.feature_importances_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''tmp = pd.DataFrame({'Feature': DecisionTreePred, 'Feature importance': DecisionTreeModel.feature_importances_})\n",
    "tmp = tmp.sort_values(by='Feature importance',ascending=False)\n",
    "plt.figure(figsize = (7,4))\n",
    "plt.title('Features importance',fontsize=14)\n",
    "s = sns.barplot(x='Feature',y='Feature importance',data=tmp)\n",
    "s.set_xticklabels(s.get_xticklabels(),rotation=90)\n",
    "plt.show()   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "#d = DecisionTreeModel.fit(X_train.values, y_train.values.copy(), 50)\n",
    "\n",
    "#preds = predict(X_test, d)\n",
    "print(classification_report(y_test,DecisionTreePred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
